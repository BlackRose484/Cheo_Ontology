import { GoogleGenerativeAI } from "@google/generative-ai";
import { OntologyQueryService } from "./ontologyQueryService";

export interface AIResponse {
  response: string;
  suggestions: string[];
  intent: string;
  confidence: number;
  model?: string;
}

export interface IntentAnalysis {
  intent: string;
  entities: string[];
  needsOntologyQuery: boolean;
  queryType?: string;
  filters?: {
    gender?: string;
    emotion?: string;
    play?: string;
    character?: string;
  };
  confidence: number;
}

export class GeminiService {
  private static genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY!);

  // Available Gemini models
  private static readonly AVAILABLE_MODELS = [
    "gemini-2.5-pro",
    "gemini-2.5-flash",
    "gemini-2.5-flash-lite",
    "gemini-2.0-flash",
    "gemini-1.5-pro",
    "gemini-1.5-flash",
  ];

  static async processCheoQuery(
    message: string,
    context: any[] = [],
    modelName = "gemini-2.5-flash"
  ): Promise<AIResponse> {
    try {
      // Validate model name
      const validModel = this.validateAndGetModel(modelName);

      // 1. Analyze intent and extract entities
      const analysis = await this.analyzeIntent(message, validModel);
      // 2. Query ontology based on intent
      let ontologyData = null;
      if (analysis.needsOntologyQuery) {
        ontologyData = await OntologyQueryService.queryByIntent(analysis);
      }

      // 3. Generate contextualized response
      const response = await this.generateResponse(
        message,
        analysis,
        ontologyData,
        context,
        validModel
      );

      return {
        response: response.text,
        suggestions: response.suggestions,
        intent: analysis.intent,
        confidence: analysis.confidence,
        model: validModel,
      };
    } catch (error) {
      console.error("Gemini processing error:", error);
      return this.getFallbackResponse(modelName);
    }
  }

  private static validateAndGetModel(modelName: string): string {
    if (this.AVAILABLE_MODELS.includes(modelName)) {
      return modelName;
    }

    console.warn(
      `Model ${modelName} not available, falling back to gemini-2.5-flash`
    );
    return "gemini-2.5-flash";
  }

  private static async analyzeIntent(
    message: string,
    modelName: string
  ): Promise<IntentAnalysis> {
    const model = this.genAI.getGenerativeModel({ model: modelName });

    const intentPrompt = `
    Ph√¢n t√≠ch √Ω ƒë·ªãnh trong c√¢u h·ªèi v·ªÅ ngh·ªá thu·∫≠t Ch√®o Vi·ªát Nam sau:
    "${message}"
    
    NGUY√äN T·∫ÆC PH√ÇN T√çCH:
    - ∆Øu ti√™n t√¨m ki·∫øm th√¥ng tin chi ti·∫øt khi c√≥ t√™n c·ª• th·ªÉ
    - T√™n nh√¢n v·∫≠t, v·ªü k·ªãch, di·ªÖn vi√™n c·ª• th·ªÉ ‚Üí intent "find_character", "find_play", "find_actor"
    - C√°c t·ª´ kh√≥a chung nh∆∞ "nh√¢n v·∫≠t n·ªØ", "di·ªÖn vi√™n nam" ‚Üí intent t∆∞∆°ng ·ª©ng v·ªõi filter
    - C√¢u h·ªèi chung v·ªÅ Ch√®o ‚Üí "general_info"
    
    Tr·∫£ v·ªÅ JSON format ch√≠nh x√°c (kh√¥ng th√™m markdown):
    {
      "intent": "find_character|find_play|find_scene|find_actor|find_emotion|general_info|greeting|help",
      "entities": ["entity1", "entity2"],
      "needsOntologyQuery": true|false,
      "queryType": "characters|plays|scenes|actors|emotions|general",
      "filters": {
        "gender": "nam|n·ªØ",
        "emotion": "vui|bu·ªìn|gi·∫≠n|s·ª£",
        "play": "t√™n v·ªü",
        "character": "t√™n nh√¢n v·∫≠t",
        "actor": "t√™n di·ªÖn vi√™n"
      },
      "confidence": 0.9
    }
    
    V√≠ d·ª•:
    - "Th·ªã M·∫ßu l√† ai?" ‚Üí intent: "find_character", entities: ["Th·ªã M·∫ßu"], needsOntologyQuery: true
    - "T√¨m nh√¢n v·∫≠t n·ªØ" ‚Üí intent: "find_character", entities: [], filters: {"gender": "n·ªØ"}
    - "V·ªü Quan √Çm Th·ªã K√≠nh" ‚Üí intent: "find_play", entities: ["Quan √Çm Th·ªã K√≠nh"]
    - "Di·ªÖn vi√™n Xu√¢n Hinh" ‚Üí intent: "find_actor", entities: ["Xu√¢n Hinh"]
    - "Ngh·ªá thu·∫≠t Ch√®o l√† g√¨" ‚Üí intent: "general_info", entities: ["Ch√®o"]
    - "Xin ch√†o" ‚Üí intent: "greeting", needsOntologyQuery: false
    `;

    try {
      const result = await model.generateContent(intentPrompt);
      const response = await result.response;
      const text = response
        .text()
        .replace(/```json\n?|\n?```/g, "")
        .trim();

      const parsed = JSON.parse(text);
      return {
        intent: parsed.intent || "general_info",
        entities: parsed.entities || [],
        needsOntologyQuery: parsed.needsOntologyQuery !== false,
        queryType: parsed.queryType,
        filters: parsed.filters || {},
        confidence: parsed.confidence || 0.5,
      };
    } catch (error) {
      console.error("Intent analysis error:", error);
      return {
        intent: "general_info",
        entities: [],
        needsOntologyQuery: true,
        confidence: 0.3,
      };
    }
  }

  private static async generateResponse(
    message: string,
    analysis: IntentAnalysis,
    ontologyData: any,
    context: any[],
    modelName: string
  ) {
    const model = this.genAI.getGenerativeModel({ model: modelName });

    const systemContext = `
    B·∫°n l√† "Ch√®o Bot" - tr·ª£ l√Ω AI chuy√™n v·ªÅ ngh·ªá thu·∫≠t Ch√®o truy·ªÅn th·ªëng Vi·ªát Nam.
    
    PERSONALITY:
    - Th√¢n thi·ªán, nhi·ªát t√¨nh, am hi·ªÉu vƒÉn h√≥a Vi·ªát Nam
    - S·ª≠ d·ª•ng ng√¥n ng·ªØ trang tr·ªçng nh∆∞ng g·∫ßn g≈©i
    - Th√™m emoji ph√π h·ª£p v·ªõi vƒÉn h√≥a Vi·ªát (üé≠, üèÆ, üå∏, üé™)
    - Khuy·∫øn kh√≠ch kh√°m ph√° th√™m v·ªÅ Ch√®o
    
    D·ªÆ LI·ªÜU T·ª™ ONTOLOGY:
    ${
      ontologyData
        ? `
Lo·∫°i d·ªØ li·ªáu: ${ontologyData.type}
S·ªë l∆∞·ª£ng k·∫øt qu·∫£: ${ontologyData.count}
D·ªØ li·ªáu chi ti·∫øt: ${JSON.stringify(ontologyData.data, null, 2)}
${ontologyData.error ? `L·ªói: ${ontologyData.error}` : ""}
        `
        : "Kh√¥ng c√≥ d·ªØ li·ªáu c·ª• th·ªÉ t·ª´ c∆° s·ªü d·ªØ li·ªáu."
    }
    
    NG·ªÆ C·∫¢NH CU·ªòC TRUY·ªÜN:
    ${
      context.length > 0
        ? JSON.stringify(context.slice(-3))
        : "ƒê√¢y l√† ƒë·∫ßu cu·ªôc tr√≤ chuy·ªán"
    }
    
    QUY T·∫ÆC TR·∫¢ L·ªúI:
    - N·∫øu c√≥ d·ªØ li·ªáu chi ti·∫øt (character_details, play_details, actor_details, scene_details), ∆∞u ti√™n tr√¨nh b√†y th√¥ng tin ƒë·∫ßy ƒë·ªß
    - V·ªõi th√¥ng tin nh√¢n v·∫≠t: t√™n, gi·ªõi t√≠nh, lo·∫°i vai, m√¥ t·∫£, c√°c v·ªü k·ªãch, di·ªÖn vi√™n ƒë√≥ng, c·∫£nh xu·∫•t hi·ªán
    - V·ªõi th√¥ng tin v·ªü k·ªãch: t√™n, t√°c gi·∫£, t√≥m t·∫Øt, s·ªë c·∫£nh, nh√¢n v·∫≠t, di·ªÖn vi√™n, danh s√°ch c·∫£nh
    - V·ªõi th√¥ng tin di·ªÖn vi√™n: t√™n, gi·ªõi t√≠nh, c√°c v·ªü ƒë√£ ƒë√≥ng, nh√¢n v·∫≠t ƒë√£ th·ªÉ hi·ªán
    - N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu ho·∫∑c c√≥ l·ªói, ƒë∆∞a ra th√¥ng tin chung v√† g·ª£i √Ω t√¨m ki·∫øm c·ª• th·ªÉ
    - Lu√¥n ƒë·ªÅ xu·∫•t 2-3 c√¢u h·ªèi ti·∫øp theo ƒë·ªÉ ng∆∞·ªùi d√πng kh√°m ph√° th√™m
    - Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, r√µ r√†ng v√† d·ªÖ hi·ªÉu
    - V·ªõi d·ªØ li·ªáu danh s√°ch, h√£y tr√¨nh b√†y c√≥ c·∫•u tr√∫c, d·ªÖ ƒë·ªçc
    `;

    const prompt = `
    ${systemContext}
    
    NG∆Ø·ªúI D√ôNG H·ªéI: "${message}"
    √ù ƒê·ªäNH PH√ÇN T√çCH: ${analysis.intent}
    
    H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch h·ªØu √≠ch v√† th√∫ v·ªã d·ª±a tr√™n d·ªØ li·ªáu c√≥ s·∫µn.
    
    Tr·∫£ v·ªÅ JSON format ch√≠nh x√°c (kh√¥ng th√™m markdown):
    {
      "text": "C√¢u tr·∫£ l·ªùi ch√≠nh v·ªõi th√¥ng tin chi ti·∫øt v√† c√≥ c·∫•u tr√∫c",
      "suggestions": ["G·ª£i √Ω c·ª• th·ªÉ 1", "G·ª£i √Ω c·ª• th·ªÉ 2", "G·ª£i √Ω c·ª• th·ªÉ 3"]
    }
    `;

    try {
      const result = await model.generateContent(prompt);
      const response = await result.response;
      const text = response
        .text()
        .replace(/```json\n?|\n?```/g, "")
        .trim();

      const parsed = JSON.parse(text);
      return {
        text: parsed.text || response.text(),
        suggestions: parsed.suggestions || [
          "T√¨m hi·ªÉu v·ªÅ nh√¢n v·∫≠t Ch√®o",
          "Kh√°m ph√° c√°c v·ªü Ch√®o n·ªïi ti·∫øng",
          "T√¨m hi·ªÉu v·ªÅ di·ªÖn vi√™n Ch√®o",
        ],
      };
    } catch (error) {
      console.error("Response generation error:", error);
      return {
        text: "T√¥i hi·ªÉu c√¢u h·ªèi c·ªßa b·∫°n v·ªÅ ngh·ªá thu·∫≠t Ch√®o. H√£y th·ª≠ h·ªèi c·ª• th·ªÉ h∆°n v·ªÅ nh√¢n v·∫≠t, v·ªü ch√®o, ho·∫∑c di·ªÖn vi√™n b·∫°n quan t√¢m! üé≠",
        suggestions: [
          "T√¨m nh√¢n v·∫≠t trong Ch√®o",
          "C√°c v·ªü Ch√®o n·ªïi ti·∫øng",
          "Di·ªÖn vi√™n Ch√®o",
        ],
      };
    }
  }

  private static getFallbackResponse(
    modelName = "gemini-2.5-flash"
  ): AIResponse {
    return {
      response:
        "Xin l·ªói, t√¥i g·∫∑p ch√∫t kh√≥ khƒÉn. B·∫°n c√≥ th·ªÉ th·ª≠ h·ªèi v·ªÅ nh√¢n v·∫≠t, v·ªü ch√®o, ho·∫∑c di·ªÖn vi√™n kh√¥ng? üé≠",
      suggestions: [
        "T√¨m nh√¢n v·∫≠t trong Ch√®o",
        "C√°c v·ªü Ch√®o n·ªïi ti·∫øng",
        "Di·ªÖn vi√™n Ch√®o",
      ],
      intent: "error",
      confidence: 0,
      model: modelName,
    };
  }

  static isAvailable(): boolean {
    return !!process.env.GEMINI_API_KEY;
  }

  static getAvailableModels(): string[] {
    return [...this.AVAILABLE_MODELS];
  }

  static isModelSupported(modelName: string): boolean {
    return this.AVAILABLE_MODELS.includes(modelName);
  }
}

import { GoogleGenerativeAI } from "@google/generative-ai";
import { OntologyQueryService } from "./ontologyQueryService";

export interface AIResponse {
  response: string;
  suggestions: string[];
  intent: string;
  confidence: number;
  model?: string;
}

export interface IntentAnalysis {
  intent: string;
  entities: string[];
  needsOntologyQuery: boolean;
  queryType?: string;
  filters?: {
    gender?: string;
    emotion?: string;
    play?: string;
    character?: string;
  };
  confidence: number;
}

export class GeminiService {
  private static genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY!);

  // Available Gemini models
  private static readonly AVAILABLE_MODELS = [
    "gemini-2.5-pro",
    "gemini-2.5-flash",
    "gemini-2.5-flash-lite",
    "gemini-2.0-flash",
    "gemini-1.5-pro",
    "gemini-1.5-flash",
  ];

  static async processCheoQuery(
    message: string,
    context: any[] = [],
    modelName = "gemini-2.5-flash"
  ): Promise<AIResponse> {
    try {
      // Validate model name
      const validModel = this.validateAndGetModel(modelName);

      // 1. Analyze intent and extract entities
      const analysis = await this.analyzeIntent(message, validModel);

      // 2. Query ontology based on intent
      let ontologyData = null;
      if (analysis.needsOntologyQuery) {
        ontologyData = await OntologyQueryService.queryByIntent(analysis);
      }

      // 3. Generate contextualized response
      const response = await this.generateResponse(
        message,
        analysis,
        ontologyData,
        context,
        validModel
      );

      return {
        response: response.text,
        suggestions: response.suggestions,
        intent: analysis.intent,
        confidence: analysis.confidence,
        model: validModel,
      };
    } catch (error) {
      console.error("Gemini processing error:", error);
      return this.getFallbackResponse(modelName);
    }
  }

  private static validateAndGetModel(modelName: string): string {
    if (this.AVAILABLE_MODELS.includes(modelName)) {
      return modelName;
    }

    console.warn(
      `Model ${modelName} not available, falling back to gemini-2.5-flash`
    );
    return "gemini-2.5-flash";
  }

  private static async analyzeIntent(
    message: string,
    modelName: string
  ): Promise<IntentAnalysis> {
    const model = this.genAI.getGenerativeModel({ model: modelName });

    const intentPrompt = `
    Ph√¢n t√≠ch √Ω ƒë·ªãnh trong c√¢u h·ªèi v·ªÅ ngh·ªá thu·∫≠t Ch√®o Vi·ªát Nam sau:
    "${message}"
    
    Tr·∫£ v·ªÅ JSON format ch√≠nh x√°c (kh√¥ng th√™m markdown):
    {
      "intent": "find_character|find_play|find_scene|find_actor|find_emotion|general_info|greeting|help",
      "entities": ["entity1", "entity2"],
      "needsOntologyQuery": true|false,
      "queryType": "characters|plays|scenes|actors|emotions|general",
      "filters": {
        "gender": "nam|n·ªØ",
        "emotion": "vui|bu·ªìn|gi·∫≠n|s·ª£",
        "play": "t√™n v·ªü",
        "character": "t√™n nh√¢n v·∫≠t"
      },
      "confidence": 0.9
    }
    
    V√≠ d·ª•:
    - "T√¨m nh√¢n v·∫≠t n·ªØ" ‚Üí intent: "find_character", entities: ["n·ªØ"], needsOntologyQuery: true
    - "Xin ch√†o" ‚Üí intent: "greeting", needsOntologyQuery: false
    - "Th·ªã M·∫ßu l√† ai" ‚Üí intent: "find_character", entities: ["Th·ªã M·∫ßu"], needsOntologyQuery: true
    `;

    try {
      const result = await model.generateContent(intentPrompt);
      const response = await result.response;
      const text = response
        .text()
        .replace(/```json\n?|\n?```/g, "")
        .trim();

      const parsed = JSON.parse(text);
      return {
        intent: parsed.intent || "general_info",
        entities: parsed.entities || [],
        needsOntologyQuery: parsed.needsOntologyQuery || false,
        queryType: parsed.queryType,
        filters: parsed.filters || {},
        confidence: parsed.confidence || 0.5,
      };
    } catch (error) {
      console.error("Intent analysis error:", error);
      return {
        intent: "general_info",
        entities: [],
        needsOntologyQuery: false,
        confidence: 0.3,
      };
    }
  }

  private static async generateResponse(
    message: string,
    analysis: IntentAnalysis,
    ontologyData: any,
    context: any[],
    modelName: string
  ) {
    const model = this.genAI.getGenerativeModel({ model: modelName });

    const systemContext = `
    B·∫°n l√† "Ch√®o Bot" - tr·ª£ l√Ω AI chuy√™n v·ªÅ ngh·ªá thu·∫≠t Ch√®o truy·ªÅn th·ªëng Vi·ªát Nam.
    
    PERSONALITY:
    - Th√¢n thi·ªán, nhi·ªát t√¨nh, am hi·ªÉu vƒÉn h√≥a Vi·ªát Nam
    - S·ª≠ d·ª•ng ng√¥n ng·ªØ trang tr·ªçng nh∆∞ng g·∫ßn g≈©i
    - Th√™m emoji ph√π h·ª£p v·ªõi vƒÉn h√≥a Vi·ªát (üé≠, üèÆ, üå∏, üé™)
    - Khuy·∫øn kh√≠ch kh√°m ph√° th√™m v·ªÅ Ch√®o
    
    D·ªÆ LI·ªÜU T·ª™ ONTOLOGY:
    ${
      ontologyData
        ? JSON.stringify(ontologyData, null, 2)
        : "Kh√¥ng c√≥ d·ªØ li·ªáu c·ª• th·ªÉ t·ª´ c∆° s·ªü d·ªØ li·ªáu."
    }
    
    NG·ªÆ C·∫¢NH CU·ªòC TRUY·ªÜN:
    ${
      context.length > 0
        ? JSON.stringify(context.slice(-3))
        : "ƒê√¢y l√† ƒë·∫ßu cu·ªôc tr√≤ chuy·ªán"
    }
    
    QUY T·∫ÆC:
    - N·∫øu c√≥ d·ªØ li·ªáu t·ª´ ontology, ∆∞u ti√™n s·ª≠ d·ª•ng th√¥ng tin n√†y
    - N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu, ƒë∆∞a ra th√¥ng tin chung v·ªÅ Ch√®o v√† g·ª£i √Ω t√¨m ki·∫øm c·ª• th·ªÉ
    - Lu√¥n ƒë·ªÅ xu·∫•t 2-3 c√¢u h·ªèi ti·∫øp theo ƒë·ªÉ ng∆∞·ªùi d√πng kh√°m ph√° th√™m
    - N·∫øu kh√¥ng ch·∫Øc ch·∫Øn, th√†nh th·∫≠t th·ª´a nh·∫≠n v√† h∆∞·ªõng d·∫´n c√°ch t√¨m th√¥ng tin
    - Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, ng·∫Øn g·ªçn nh∆∞ng ƒë·∫ßy ƒë·ªß th√¥ng tin
    `;

    const prompt = `
    ${systemContext}
    
    NG∆Ø·ªúI D√ôNG H·ªéI: "${message}"
    √ù ƒê·ªäNH PH√ÇN T√çCH: ${analysis.intent}
    
    H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch h·ªØu √≠ch v√† th√∫ v·ªã.
    
    Tr·∫£ v·ªÅ JSON format ch√≠nh x√°c (kh√¥ng th√™m markdown):
    {
      "text": "C√¢u tr·∫£ l·ªùi ch√≠nh",
      "suggestions": ["G·ª£i √Ω 1", "G·ª£i √Ω 2", "G·ª£i √Ω 3"]
    }
    `;

    try {
      const result = await model.generateContent(prompt);
      const response = await result.response;
      const text = response
        .text()
        .replace(/```json\n?|\n?```/g, "")
        .trim();

      const parsed = JSON.parse(text);
      return {
        text: parsed.text || response.text(),
        suggestions: parsed.suggestions || [
          "T√¨m hi·ªÉu v·ªÅ nh√¢n v·∫≠t Ch√®o",
          "Kh√°m ph√° c√°c v·ªü Ch√®o n·ªïi ti·∫øng",
          "T√¨m hi·ªÉu v·ªÅ di·ªÖn vi√™n Ch√®o",
        ],
      };
    } catch (error) {
      console.error("Response generation error:", error);
      return {
        text: "T√¥i hi·ªÉu c√¢u h·ªèi c·ªßa b·∫°n v·ªÅ ngh·ªá thu·∫≠t Ch√®o. H√£y th·ª≠ h·ªèi c·ª• th·ªÉ h∆°n v·ªÅ nh√¢n v·∫≠t, v·ªü ch√®o, ho·∫∑c di·ªÖn vi√™n b·∫°n quan t√¢m! üé≠",
        suggestions: [
          "T√¨m nh√¢n v·∫≠t trong Ch√®o",
          "C√°c v·ªü Ch√®o n·ªïi ti·∫øng",
          "Di·ªÖn vi√™n Ch√®o",
        ],
      };
    }
  }

  private static getFallbackResponse(
    modelName = "gemini-2.5-flash"
  ): AIResponse {
    return {
      response:
        "Xin l·ªói, t√¥i g·∫∑p ch√∫t kh√≥ khƒÉn. B·∫°n c√≥ th·ªÉ th·ª≠ h·ªèi v·ªÅ nh√¢n v·∫≠t, v·ªü ch√®o, ho·∫∑c di·ªÖn vi√™n kh√¥ng? üé≠",
      suggestions: [
        "T√¨m nh√¢n v·∫≠t trong Ch√®o",
        "C√°c v·ªü Ch√®o n·ªïi ti·∫øng",
        "Di·ªÖn vi√™n Ch√®o",
      ],
      intent: "error",
      confidence: 0,
      model: modelName,
    };
  }

  static isAvailable(): boolean {
    return !!process.env.GEMINI_API_KEY;
  }

  static getAvailableModels(): string[] {
    return [...this.AVAILABLE_MODELS];
  }

  static isModelSupported(modelName: string): boolean {
    return this.AVAILABLE_MODELS.includes(modelName);
  }
}
